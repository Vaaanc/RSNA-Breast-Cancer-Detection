{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39272,"databundleVersionId":4629629,"sourceType":"competition"},{"sourceId":4619402,"sourceType":"datasetVersion","datasetId":2688773},{"sourceId":4619805,"sourceType":"datasetVersion","datasetId":2688675}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pydicom\nimport cv2\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torchinfo import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-13T22:27:00.571170Z","iopub.execute_input":"2023-11-13T22:27:00.572030Z","iopub.status.idle":"2023-11-13T22:27:05.948421Z","shell.execute_reply.started":"2023-11-13T22:27:00.571979Z","shell.execute_reply":"2023-11-13T22:27:05.947407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\ndf_train = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:05.950593Z","iopub.execute_input":"2023-11-13T22:27:05.951234Z","iopub.status.idle":"2023-11-13T22:27:06.104116Z","shell.execute_reply.started":"2023-11-13T22:27:05.951193Z","shell.execute_reply":"2023-11-13T22:27:06.102845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:06.105825Z","iopub.execute_input":"2023-11-13T22:27:06.106547Z","iopub.status.idle":"2023-11-13T22:27:06.134754Z","shell.execute_reply.started":"2023-11-13T22:27:06.106485Z","shell.execute_reply":"2023-11-13T22:27:06.133543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:06.137336Z","iopub.execute_input":"2023-11-13T22:27:06.137869Z","iopub.status.idle":"2023-11-13T22:27:06.165005Z","shell.execute_reply.started":"2023-11-13T22:27:06.137826Z","shell.execute_reply":"2023-11-13T22:27:06.163928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.drop(['BIRADS', 'density'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:06.166550Z","iopub.execute_input":"2023-11-13T22:27:06.167049Z","iopub.status.idle":"2023-11-13T22:27:06.183572Z","shell.execute_reply.started":"2023-11-13T22:27:06.167008Z","shell.execute_reply":"2023-11-13T22:27:06.182168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train['age'].isna()] = int(df_train['age'].mean())","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:06.185521Z","iopub.execute_input":"2023-11-13T22:27:06.186100Z","iopub.status.idle":"2023-11-13T22:27:06.198175Z","shell.execute_reply.started":"2023-11-13T22:27:06.186057Z","shell.execute_reply":"2023-11-13T22:27:06.196934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:06.199693Z","iopub.execute_input":"2023-11-13T22:27:06.200246Z","iopub.status.idle":"2023-11-13T22:27:06.228563Z","shell.execute_reply.started":"2023-11-13T22:27:06.200214Z","shell.execute_reply":"2023-11-13T22:27:06.227476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10,4))\nsns.histplot(df_train, x='cancer',  bins=2, ax=axes[0])\naxes[0].set_xticks([0, 1])\n\nres_counts = df_train['cancer'].value_counts()\naxes[1] = plt.pie(res_counts, labels=res_counts.index, autopct='%1.1f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:06.230417Z","iopub.execute_input":"2023-11-13T22:27:06.231183Z","iopub.status.idle":"2023-11-13T22:27:06.651464Z","shell.execute_reply.started":"2023-11-13T22:27:06.231149Z","shell.execute_reply":"2023-11-13T22:27:06.650281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15, 3))\n\nsns.kdeplot(data=df_train, x='age', hue='cancer', fill=True, common_norm=False, ax=axes[0])\naxes[0].set_title('Age vs. Cancer')\n\nno_cancer = df_train[df_train['cancer'] == 0]['age']\ncancer_with_invasive = df_train[(df_train['cancer'] == 1) & (df_train['invasive'] == 1)]['age']\ncancer_no_invasive = df_train[(df_train['cancer'] == 1) & (df_train['invasive'] == 0)]['age']\n\nsns.kdeplot(no_cancer, label='No Cancer', fill=True, ax=axes[1])\nsns.kdeplot(cancer_with_invasive, label='Cancer with Invasive', fill=True, ax=axes[1])\nsns.kdeplot(cancer_no_invasive, label='Cancer without Invasive', fill=True, ax=axes[1])\n\naxes[1].set_title('Age vs cancer with invasiveness')\naxes[1].set_xlabel('Age')\naxes[1].set_ylabel('Density')\nlegend = axes[1].legend()\nlegend.set_title('Conditions')\n\nplt.show","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:06.652872Z","iopub.execute_input":"2023-11-13T22:27:06.653278Z","iopub.status.idle":"2023-11-13T22:27:08.192018Z","shell.execute_reply.started":"2023-11-13T22:27:06.653241Z","shell.execute_reply":"2023-11-13T22:27:08.191064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dicom_file_path = \"/kaggle/input/rsna-breast-cancer-detection/train_images/10006/462822612.dcm\"\nds = pydicom.dcmread(dicom_file_path)\nprint(ds)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:08.196464Z","iopub.execute_input":"2023-11-13T22:27:08.196847Z","iopub.status.idle":"2023-11-13T22:27:08.249742Z","shell.execute_reply.started":"2023-11-13T22:27:08.196815Z","shell.execute_reply":"2023-11-13T22:27:08.248583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 'PixelData' in ds:\n    import matplotlib.pyplot as plt\n    plt.imshow(ds.pixel_array, cmap='gray')\n    plt.show()\nelse:\n    print(\"This DICOM file does not contain any images.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:08.251603Z","iopub.execute_input":"2023-11-13T22:27:08.252035Z","iopub.status.idle":"2023-11-13T22:27:12.538445Z","shell.execute_reply.started":"2023-11-13T22:27:08.251985Z","shell.execute_reply":"2023-11-13T22:27:12.537309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_data = ds.pixel_array\n\nfig, axes = plt.subplots(1, 2, figsize=(7,7))\n\ncropped_image = image_data[:, :int(ds.WindowCenter[0])]\naxes[0].imshow(cropped_image, cmap='gray')\naxes[0].set_xlabel(f'ds.WindowCenter[0]')\n\n# Crop the image using the line position\ncropped_image = image_data[:, :int(ds.WindowCenter[3])]\naxes[1].imshow(cropped_image, cmap='gray')\naxes[1].set_xlabel(f'ds.WindowCenter[3]')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:12.539940Z","iopub.execute_input":"2023-11-13T22:27:12.540354Z","iopub.status.idle":"2023-11-13T22:27:14.045828Z","shell.execute_reply.started":"2023-11-13T22:27:12.540314Z","shell.execute_reply":"2023-11-13T22:27:14.044645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dicom_file_path = \"/kaggle/input/rsna-breast-cancer-detection/train_images/10006/1874946579.dcm\"\nds = pydicom.dcmread(dicom_file_path)\nprint(ds)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:14.047607Z","iopub.execute_input":"2023-11-13T22:27:14.048415Z","iopub.status.idle":"2023-11-13T22:27:14.099708Z","shell.execute_reply.started":"2023-11-13T22:27:14.048372Z","shell.execute_reply":"2023-11-13T22:27:14.098571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int(ds.WindowCenter[3]/10)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:14.101301Z","iopub.execute_input":"2023-11-13T22:27:14.102814Z","iopub.status.idle":"2023-11-13T22:27:14.110644Z","shell.execute_reply.started":"2023-11-13T22:27:14.102768Z","shell.execute_reply":"2023-11-13T22:27:14.109552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('/kaggle/input/rsna-breast-cancer-256-pngs/10006_1459541791.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# plt.imshow(img[:, :int(ds.WindowCenter[3]/18)], cmap='gray')\nplt.imshow(img, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:14.112618Z","iopub.execute_input":"2023-11-13T22:27:14.112960Z","iopub.status.idle":"2023-11-13T22:27:14.455189Z","shell.execute_reply.started":"2023-11-13T22:27:14.112932Z","shell.execute_reply":"2023-11-13T22:27:14.454106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downsample","metadata":{}},{"cell_type":"code","source":"df_target1 = df_train[df_train['cancer'] == 1]\ndf_target0 = df_train[df_train['cancer'] == 0]\n\nn_idx0 = len(df_target0)\nn_idx1 = len(df_target1)\n\ndf_target0_downsampled = df_target0.sample(n=n_idx1, random_state=42)\ndf_downsampled = pd.concat([df_target0_downsampled, df_target1])\ndf_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:14.456677Z","iopub.execute_input":"2023-11-13T22:27:14.457619Z","iopub.status.idle":"2023-11-13T22:27:14.478172Z","shell.execute_reply.started":"2023-11-13T22:27:14.457586Z","shell.execute_reply":"2023-11-13T22:27:14.477083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(df_downsampled[df_downsampled['cancer'] == 0]))\nprint(len(df_downsampled[df_downsampled['cancer'] == 1]))","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:14.482848Z","iopub.execute_input":"2023-11-13T22:27:14.483239Z","iopub.status.idle":"2023-11-13T22:27:14.492357Z","shell.execute_reply.started":"2023-11-13T22:27:14.483206Z","shell.execute_reply":"2023-11-13T22:27:14.491415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_train_downsampled, df_val_downsampled = train_test_split(df_downsampled, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:16.179560Z","iopub.execute_input":"2023-11-13T22:27:16.180385Z","iopub.status.idle":"2023-11-13T22:27:16.365257Z","shell.execute_reply.started":"2023-11-13T22:27:16.180340Z","shell.execute_reply":"2023-11-13T22:27:16.363998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BreastDataset(Dataset):\n    def __init__(self, df, img_size, image_dir, transform=None, additional_transforms=None):\n        super(BreastDataset, self).__init__()\n        \n        self.df = df\n        self.img_size = img_size\n        self.image_dir = image_dir\n        self.transform = transform\n        self.additional_transforms = additional_transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def  __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        label = self.df.iloc[idx]['cancer']\n        file_name = str(row['patient_id']) + \"_\" + str(row['image_id']) + \".png\"\n        image_path = self.image_dir + file_name\n        \n        image = Image.open(image_path).convert('RGB')\n#         image = cv2.imread(image_path)\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            image = transform(image)\n        \n        if label == 1 and self.additional_transforms:\n            image = additional_transform(image)\n            \n        return image, label\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:16.447441Z","iopub.execute_input":"2023-11-13T22:27:16.447861Z","iopub.status.idle":"2023-11-13T22:27:16.458563Z","shell.execute_reply.started":"2023-11-13T22:27:16.447828Z","shell.execute_reply":"2023-11-13T22:27:16.457214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path_256 = '/kaggle/input/rsna-breast-cancer-512-pngs/'\nimage_size = (512, 512)\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nadditional_transform = transforms.RandomApply([\n    transforms.RandomRotation(degrees=(-20, 20))],\n    p=0.5)\n\nds_train = BreastDataset(df_train_downsampled, image_size, img_path_256, transform, additional_transform)\nds_val = BreastDataset(df_val_downsampled, image_size, img_path_256, transform, None)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:03:33.545262Z","iopub.execute_input":"2023-11-14T00:03:33.545750Z","iopub.status.idle":"2023-11-14T00:03:33.554152Z","shell.execute_reply.started":"2023-11-14T00:03:33.545713Z","shell.execute_reply":"2023-11-14T00:03:33.552907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:03:34.844160Z","iopub.execute_input":"2023-11-14T00:03:34.844579Z","iopub.status.idle":"2023-11-14T00:03:34.850207Z","shell.execute_reply.started":"2023-11-14T00:03:34.844546Z","shell.execute_reply":"2023-11-14T00:03:34.849345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(ds_train[0][0].numpy().transpose(1,2,0), cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:03:35.977562Z","iopub.execute_input":"2023-11-14T00:03:35.978549Z","iopub.status.idle":"2023-11-14T00:03:35.983103Z","shell.execute_reply.started":"2023-11-14T00:03:35.978485Z","shell.execute_reply":"2023-11-14T00:03:35.982021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"IMAGE_WIDTH     = image_size[0]\nIMAGE_HEIGHT    = image_size[1]\nIMAGE_CHANNELS  = 3\nDEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:05:01.559466Z","iopub.execute_input":"2023-11-14T00:05:01.559879Z","iopub.status.idle":"2023-11-14T00:05:01.566407Z","shell.execute_reply.started":"2023-11-14T00:05:01.559848Z","shell.execute_reply":"2023-11-14T00:05:01.564963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_maxpool(size, stride, kernel_size):\n    return (size - kernel_size) // stride + 1","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:03:38.797285Z","iopub.execute_input":"2023-11-14T00:03:38.797735Z","iopub.status.idle":"2023-11-14T00:03:38.804155Z","shell.execute_reply.started":"2023-11-14T00:03:38.797698Z","shell.execute_reply":"2023-11-14T00:03:38.802671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_maxpool(254, 2, 2) # 3\ncalc_maxpool(125, 2, 2) # 32\ncalc_maxpool(60, 2, 2) # 64\ncalc_maxpool(28, 2, 2)  # 128\ncalc_maxpool(12, 2, 2) # 256\ncalc_maxpool(10, 2, 2) # 512\ncalc_maxpool(3, 2, 1) # 512","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:03:44.656865Z","iopub.execute_input":"2023-11-14T00:03:44.657443Z","iopub.status.idle":"2023-11-14T00:03:44.667405Z","shell.execute_reply.started":"2023-11-14T00:03:44.657407Z","shell.execute_reply":"2023-11-14T00:03:44.666428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_output_size_with_dilation(input_size, kernel_size, padding, stride=1, dilation=1):\n    return (input_size + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n#     return (input_size - kernel_size + (kernel_size - 1) * (dilation - 1) + 2 * padding) // stride + 1","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:03:45.444721Z","iopub.execute_input":"2023-11-14T00:03:45.445622Z","iopub.status.idle":"2023-11-14T00:03:45.451975Z","shell.execute_reply.started":"2023-11-14T00:03:45.445586Z","shell.execute_reply":"2023-11-14T00:03:45.451001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_output_size_with_dilation(input_size=256, kernel_size=3, padding=0, stride=1, dilation=1)\ncalculate_output_size_with_dilation(input_size=512, kernel_size=3, padding=0, stride=1, dilation=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:03:55.961324Z","iopub.execute_input":"2023-11-14T00:03:55.961750Z","iopub.status.idle":"2023-11-14T00:03:55.970746Z","shell.execute_reply.started":"2023-11-14T00:03:55.961714Z","shell.execute_reply":"2023-11-14T00:03:55.969559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_output_size_with_dilation(input_size=256, kernel_size=3, padding=1, stride=1, dilation=2)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:27:46.832017Z","iopub.execute_input":"2023-11-13T22:27:46.832431Z","iopub.status.idle":"2023-11-13T22:27:46.840288Z","shell.execute_reply.started":"2023-11-13T22:27:46.832399Z","shell.execute_reply":"2023-11-13T22:27:46.839122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, in_channels=[3, 32, 64, 128, 256, 512, 512]):\n        super(CNN, self).__init__()\n        \n        self.pool = nn.MaxPool2d(2, 2)\n        self.last_pool = nn.MaxPool2d(2, 1)\n        self.blocks = nn.ModuleList()\n        for idx, i in enumerate(in_channels):\n            if i != in_channels[-1]:\n                out_channels = in_channels[idx+1]\n                pool = self.pool\n            else:\n                out_channels = in_channels[-1]\n                pool = None\n            \n            self.blocks.append(ConvStridedBlock(i, out_channels, 3, 0, 1, 2))\n            if pool:\n                self.blocks.append(pool)\n        \n        self.fc1 = nn.Linear(512 * 10 * 10, 4096)\n        self.fc2 = nn.Linear(4096, 2048)\n        self.fc3 = nn.Linear(2048, 1)\n        self.sigmoid = nn.Sigmoid()\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        for block in self.blocks:\n            x = block(x)\n            \n        x = x.view(batch_size, -1)\n        \n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return self.sigmoid(x)\n    \nclass ConvStridedBlock(nn.Module):\n    def __init__(self, input_size, out_channels, kernel_size, padding, stride, dilation):\n        super(ConvStridedBlock, self).__init__()\n        \n        self.cnn1 = nn.Conv2d(in_channels=input_size, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride, dilation=1, bias=False)\n        self.batchNorm = nn.BatchNorm2d(out_channels)\n        self.cnn2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1, stride=stride, dilation=1, bias=False)\n        self.dilated = nn.Conv2d(in_channels=input_size, out_channels=out_channels, kernel_size=kernel_size, padding=1, stride=stride, dilation=dilation, bias=False)\n        self.dropout = nn.Dropout(0.2)\n        self.gelu = nn.GELU()\n        self.leaky = nn.LeakyReLU()\n        \n    def forward(self, x):\n        residual = x\n        x = self.cnn1(x)\n        x = self.batchNorm(x)\n#         x = self.leaky(x)\n        \n        x = self.cnn2(x)\n        x = self.batchNorm(x)\n#         x = self.leaky(x)\n        \n        residual = self.dilated(residual)\n        x = x + residual\n        x = self.leaky(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:48:43.757381Z","iopub.execute_input":"2023-11-14T00:48:43.757780Z","iopub.status.idle":"2023-11-14T00:48:43.775954Z","shell.execute_reply.started":"2023-11-14T00:48:43.757748Z","shell.execute_reply":"2023-11-14T00:48:43.775060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csb = ConvStridedBlock(3, 32, 3, 0, 1, 2)\n\nsummary(model=csb,\n        input_size=(1, 3, 512, 512), # (batch_size, num_patches, embedding_dimension)\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:48:44.474266Z","iopub.execute_input":"2023-11-14T00:48:44.475159Z","iopub.status.idle":"2023-11-14T00:48:44.568300Z","shell.execute_reply.started":"2023-11-14T00:48:44.475107Z","shell.execute_reply":"2023-11-14T00:48:44.567189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CNN()\n\nsummary(model=model,\n        input_size=(1, 3, 512, 512), # (batch_size, num_patches, embedding_dimension)\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:48:45.363538Z","iopub.execute_input":"2023-11-14T00:48:45.363954Z","iopub.status.idle":"2023-11-14T00:48:48.375686Z","shell.execute_reply.started":"2023-11-14T00:48:45.363919Z","shell.execute_reply":"2023-11-14T00:48:48.374563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vision Transformer","metadata":{}},{"cell_type":"code","source":"PATCH_SIZE      = 16\nIMAGE_WIDTH     = image_size[0]\nIMAGE_HEIGHT    = image_size[1]\nIMAGE_CHANNELS  = 3\nEMBEDDING_DIMS  = IMAGE_CHANNELS * PATCH_SIZE**2 # 768\nNUM_OF_PATCHES  = int((IMAGE_WIDTH * IMAGE_HEIGHT) / PATCH_SIZE**2) # 256\nDEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nassert IMAGE_WIDTH % PATCH_SIZE == 0 and IMAGE_HEIGHT % PATCH_SIZE ==0 , print(\"Image Width is not divisible by patch size\")","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:28:50.036231Z","iopub.execute_input":"2023-11-13T22:28:50.036666Z","iopub.status.idle":"2023-11-13T22:28:50.043544Z","shell.execute_reply.started":"2023-11-13T22:28:50.036628Z","shell.execute_reply":"2023-11-13T22:28:50.042345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(EMBEDDING_DIMS)\nprint(NUM_OF_PATCHES)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:28:51.526454Z","iopub.execute_input":"2023-11-13T22:28:51.526876Z","iopub.status.idle":"2023-11-13T22:28:51.532655Z","shell.execute_reply.started":"2023-11-13T22:28:51.526844Z","shell.execute_reply":"2023-11-13T22:28:51.531531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MachineLearningPerceptronBlock(nn.Module):\n    def __init__(self, embedding_dims, mlp_size, mlp_dropout):\n        super().__init__()\n        self.embedding_dims = embedding_dims\n        self.mlp_size = mlp_size\n        self.dropout = mlp_dropout\n\n        self.layernorm = nn.LayerNorm(normalized_shape = embedding_dims)\n        self.mlp = nn.Sequential(\n            nn.Linear(in_features = embedding_dims, out_features = mlp_size),\n            nn.GELU(),\n            nn.Dropout(p = mlp_dropout),\n            nn.Linear(in_features = mlp_size, out_features = embedding_dims),\n            nn.Dropout(p = mlp_dropout)\n        )\n\n    def forward(self, x):\n        return self.mlp(self.layernorm(x))\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self,\n                 embedding_dims = EMBEDDING_DIMS, # Hidden Size D in the ViT Paper Table 1\n                 num_heads = 8,  # Heads in the ViT Paper Table 1\n                 attn_dropout = 0.0,\n                 mlp_size = 3072,\n                 mlp_dropout = 0.1): # Default to Zero as there is no dropout for the the MSA Block as per the ViT Paper):\n        super(TransformerEncoder, self).__init__()\n                 \n        self.layerNorm = nn.LayerNorm(EMBEDDING_DIMS)\n        self.multiheadattention = nn.MultiheadAttention(num_heads = num_heads, \n                                               embed_dim = embedding_dims, \n                                               dropout = attn_dropout, \n                                               batch_first = True)\n        self.mlp = MachineLearningPerceptronBlock(embedding_dims = embedding_dims, \n                                                  mlp_size = mlp_size,\n                                                  mlp_dropout = mlp_dropout)\n        \n    def forward(self, x):\n        x2 = self.layerNorm(x)\n        output, _ = self.multiheadattention(query=x2, key=x2, value=x2, need_weights=False)\n        \n        x = x + x2\n        \n        x2 = self.layerNorm(x)\n        x2 = self.mlp(x2)\n        \n        x = x + x2\n        return x\n    \n    \nclass SampleViT(nn.Module):\n    def __init__(self, img_size = IMAGE_HEIGHT,\n                 in_channels = IMAGE_CHANNELS,\n                 patch_size = PATCH_SIZE,\n                 embedding_dims = EMBEDDING_DIMS,\n                 num_transformer_layers = 8, # from table 1 above\n                 mlp_dropout = 0.1,\n                 attn_dropout = 0.0,\n                 mlp_size = 3072,\n                 num_heads = 12,\n                 num_classes = 1):\n        super(SampleViT, self).__init__()\n        \n        self.positional_embedding = nn.Parameter(torch.randn(1, NUM_OF_PATCHES + 1, EMBEDDING_DIMS))\n        self.class_token = nn.Parameter(torch.rand((1, 1, EMBEDDING_DIMS), requires_grad=True))\n\n        self.patchConv = conv_layer = nn.Conv2d(in_channels = IMAGE_CHANNELS, out_channels = EMBEDDING_DIMS, kernel_size = PATCH_SIZE, stride = PATCH_SIZE)\n        self.transformer_encoder = nn.Sequential(*[TransformerEncoder(embedding_dims = embedding_dims,\n                                              mlp_dropout = mlp_dropout,\n                                              attn_dropout = attn_dropout,\n                                              mlp_size = mlp_size,\n                                              num_heads = num_heads) for _ in range(num_transformer_layers)])\n\n        self.classifier = nn.Sequential(nn.LayerNorm(normalized_shape = embedding_dims),\n                                    nn.Linear(in_features = embedding_dims,\n                                              out_features = num_classes))\n    \n    def patch(self, images):\n        batch_size, c, h, w = images.shape\n        assert h == w # Square image only\n            \n        patch = self.patchConv(images)\n        patch = patch.view(batch_size, patch.shape[1], -1).permute(0, 2, 1)\n        \n        return patch\n    \n    def forward(self, x):\n        patch = self.patch(x)\n#         class_token = nn.Parameter(torch.rand((x.size(0), 1,EMBEDDING_DIMS), requires_grad  = True)).to(DEVICE)\n        class_token = self.class_token.expand(x.size(0), -1, -1)  # Expand class token for each element in the batch\n        embedded_image_with_class_token_embeddings = torch.cat((class_token, patch), dim=1)\n        embedded_patch = embedded_image_with_class_token_embeddings + self.positional_embedding\n        return torch.sigmoid(self.classifier(self.transformer_encoder(embedded_patch)[:, 0]))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:28:52.203459Z","iopub.execute_input":"2023-11-13T22:28:52.204770Z","iopub.status.idle":"2023-11-13T22:28:52.226745Z","shell.execute_reply.started":"2023-11-13T22:28:52.204723Z","shell.execute_reply":"2023-11-13T22:28:52.225557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SampleViT().to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T00:23:24.003293Z","iopub.execute_input":"2023-11-12T00:23:24.004212Z","iopub.status.idle":"2023-11-12T00:23:24.525317Z","shell.execute_reply.started":"2023-11-12T00:23:24.004178Z","shell.execute_reply":"2023-11-12T00:23:24.524549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp = MachineLearningPerceptronBlock(embedding_dims = EMBEDDING_DIMS, \n                                     mlp_size = 3072,\n                                     mlp_dropout = 0.1)\n\nsummary(model=mlp,\n        input_size=(5, 256, 768), # (batch_size, num_patches, embedding_dimension)\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-11T19:55:42.296143Z","iopub.execute_input":"2023-11-11T19:55:42.296540Z","iopub.status.idle":"2023-11-11T19:55:42.448421Z","shell.execute_reply.started":"2023-11-11T19:55:42.296509Z","shell.execute_reply":"2023-11-11T19:55:42.446976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"te = TransformerEncoder()\n\nsummary(model=te,\n        input_size=(5, 256, 768), # (batch_size, num_patches, embedding_dimension)\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-11T19:55:43.454086Z","iopub.execute_input":"2023-11-11T19:55:43.454692Z","iopub.status.idle":"2023-11-11T19:55:43.677912Z","shell.execute_reply.started":"2023-11-11T19:55:43.454656Z","shell.execute_reply":"2023-11-11T19:55:43.676520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=model,\n        input_size=(1, 3, 256, 256), # (batch_size, num_patches, embedding_dimension)\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T00:22:22.889420Z","iopub.execute_input":"2023-11-12T00:22:22.890101Z","iopub.status.idle":"2023-11-12T00:22:27.241017Z","shell.execute_reply.started":"2023-11-12T00:22:22.890067Z","shell.execute_reply":"2023-11-12T00:22:27.240012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.00001, weight_decay=1e-4)\nnum_epochs = 20\n\n# Training loop\ndef train_model(model, train_loader, criterion, optimizer, num_epochs=20):\n    model.to(DEVICE)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch + 1}/{num_epochs}')\n\n        total_loss = 0\n        correct_predictions = 0\n        total_samples = 0\n\n        for batch_idx, (batch_inputs, batch_labels) in enumerate(loop):\n            batch_inputs, batch_labels = batch_inputs.to(DEVICE), batch_labels.to(DEVICE)\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(batch_inputs)\n\n            # Compute loss\n            loss = criterion(outputs.squeeze(), batch_labels.float())\n\n            # Backward pass\n            loss.backward()\n\n            # Update weights\n            optimizer.step()\n#             loop.set_postfix(\n#                 loss=loss.item(),\n#             )\n\n            total_loss += loss.item()\n            predicted_labels = torch.round(outputs)  # Assuming sigmoid activation for binary classification\n            correct_predictions += (predicted_labels == batch_labels).sum().item()\n            total_samples += batch_labels.size(0)\n\n            # Update tqdm bar description with the current loss and accuracy\n            loop.set_postfix(loss=total_loss / (batch_idx + 1), accuracy=correct_predictions / total_samples)\n\n        accuracy = correct_predictions / total_samples\n        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}, Accuracy: {accuracy}%')","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:48:49.156838Z","iopub.execute_input":"2023-11-14T00:48:49.157491Z","iopub.status.idle":"2023-11-14T00:48:49.168747Z","shell.execute_reply.started":"2023-11-14T00:48:49.157458Z","shell.execute_reply":"2023-11-14T00:48:49.167928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(model, train_loader, criterion, optimizer, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T00:48:49.395630Z","iopub.execute_input":"2023-11-14T00:48:49.396253Z","iopub.status.idle":"2023-11-14T05:55:00.990107Z","shell.execute_reply.started":"2023-11-14T00:48:49.396220Z","shell.execute_reply":"2023-11-14T05:55:00.987153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}